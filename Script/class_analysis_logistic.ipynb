{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify SHAP in logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python一般\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import importlib\n",
    "import pickle\n",
    "import joblib\n",
    "import sklearn\n",
    "import datetime\n",
    "import copy\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.model_selection import train_test_split, KFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, brier_score_loss, roc_auc_score, roc_curve, precision_recall_fscore_support\n",
    "import torch\n",
    "import torchtuples as tt\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "import dgl.nn.pytorch as dglnn\n",
    "\n",
    "import shap\n",
    "\n",
    "import module.utils as utils\n",
    "import module.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%precision 4\n",
    "\n",
    "ROOT= \"/\".join(os.getcwd().split(\"/\")[:-1])\n",
    "DIR_LOG = os.path.join(ROOT, \"LOG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data File\n",
    "gmt_path = os.path.join(ROOT, \"data\", \"GSEA\", \"geneset\", \"c2.cp.kegg.v7.3.symbols.gmt\")\n",
    "gml_path = os.path.join(ROOT, \"data\", \"Graphml\", \"kegg.graphml\")\n",
    "ensembl_path = os.path.join(ROOT, \"data\", \"GSEA\", \"geneset\", \"Human_ENSEMBL_Gene_ID_MSigDB.v7.3.chip\")\n",
    "entrez_path = os.path.join(ROOT, \"data\", \"GSEA\", \"geneset\", \"Human_NCBI_Entrez_Gene_ID_MSigDB.v7.2.chip\")\n",
    "expression_path = os.path.join(ROOT, \"data\", \"GSE31312\", \"rma_expression.pickle\")\n",
    "clinical_df_path = os.path.join(ROOT, \"data\", \"GSE31312\", \"clinical.csv\")\n",
    "\n",
    "cv = 5\n",
    "n_seed = 1234\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "np.random.seed(n_seed)\n",
    "_ = torch.manual_seed(n_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "from module.models import Graph_from_GSEA, Load_Dataset, Graph_Exp_Data\n",
    "# Graphの読み込み(Reactome)\n",
    "path_graphs = Graph_from_GSEA(gmt_path = gmt_path,\n",
    "                              gml_path = gml_path,\n",
    "                              gene_convert_ensembl = ensembl_path,\n",
    "                              gene_convert_entrez  = entrez_path)\n",
    "\n",
    "# GSEのexpression dataの読み込み\n",
    "dataset = Load_Dataset(array_path = expression_path,\n",
    "                   clinical_path = clinical_df_path)\n",
    "\n",
    "#ラベルデータを取得\n",
    "dataset.Y = pd.DataFrame(dataset.clinicalDf[\"GEP\"].map({\"GCB\":0, \"ABC\":1, np.nan:np.nan}).dropna())\n",
    "\n",
    "# グラフ上の遺伝子のみを残す\n",
    "dataset.x_expression = dataset.x_expression.loc[:, np.isin(dataset.x_expression.columns, list(path_graphs.pathways.all_genes))]\n",
    "dataset.x_expression = pd.DataFrame(scipy.stats.zscore(dataset.x_expression, axis=1), \n",
    "                             index=dataset.x_expression.index, \n",
    "                             columns=dataset.x_expression.columns) # 正規化\n",
    "train_idx = dataset.Y.index\n",
    "\n",
    "dgl_dataset = Graph_Exp_Data(x=dataset.x_expression.loc[train_idx, :], \n",
    "                             y=dataset.Y.loc[train_idx], \n",
    "                             graphs = path_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スコアリング用データセットの読み込み\n",
    "expression_path = os.path.join(ROOT, \"data\", \"GSE10846\", \"rma_expression.pickle\")\n",
    "clinical_df_path = os.path.join(ROOT, \"data\", \"GSE10846\", \"clinical.csv\")\n",
    "\n",
    "test_dataset = Load_Dataset(array_path = expression_path,\n",
    "                   clinical_path = clinical_df_path)\n",
    "\n",
    "test_dataset.Y = pd.DataFrame(test_dataset.clinicalDf[\"GEP\"].map({\"GCB\":0, \"ABC\":1, \"UC\": np.nan, np.nan:np.nan}).dropna())\n",
    "\n",
    "# グラフ上の遺伝子のみを残す\n",
    "test_dataset.x_expression = test_dataset.x_expression.loc[:, np.isin(test_dataset.x_expression.columns, list(path_graphs.pathways.all_genes))]\n",
    "test_dataset.x_expression = pd.DataFrame(scipy.stats.zscore(test_dataset.x_expression, axis=1), \n",
    "                             index=test_dataset.x_expression.index, \n",
    "                             columns=test_dataset.x_expression.columns) # 正規化\n",
    "\n",
    "test_idx = test_dataset.Y.index\n",
    "\n",
    "test_dgl_dataset = Graph_Exp_Data(x=test_dataset.x_expression.loc[test_idx, :], \n",
    "                             y=test_dataset.Y.loc[test_idx], \n",
    "                             graphs = path_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 保存されたデータの読み込み\n",
    "モデルとSHAPの記録を読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl_dataset.mapping_attr()\n",
    "X_train = dgl_dataset.attr.float()\n",
    "Y_train = dgl_dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(models)\n",
    "from module.models import GCN_Trainer\n",
    "torch.manual_seed(n_seed)\n",
    "variable_name = \"gcn\"\n",
    "file_path = os.path.join(DIR_LOG, str(n_seed), \"gcn\")\n",
    "trainer = GCN_Trainer(dgl_dataset.batched_graph, path=file_path, device=device)\n",
    "trainer.load_bestmodel(load_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_value_df = pd.read_csv(os.path.join(file_path, \"kegg_shap.csv\"), index_col=0)\n",
    "shap_cols = [col.split(\".\")[0] for col in shap_value_df.columns]\n",
    "shap_value_df = shap_value_df.abs().T.groupby(shap_cols).sum().T\n",
    "shap.summary_plot(shap_value_df, feature_names=shap_value_df.columns, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression classifier for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression using all genes in KEGG pathways\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    "\n",
    "n_start=0\n",
    "print(\"=================\")\n",
    "print(f\"n_start: {n_start}\")\n",
    "print(\"-----------------\")\n",
    "\n",
    "pathway_names = list(shap_value_df.mean().sort_values(ascending=False).index)\n",
    "gene_names = set()\n",
    "for p in pathway_names:\n",
    "    gene_names = gene_names | set(path_graphs.id_to_symbol[p].values())\n",
    "print(f\"Number of genes: {len(gene_names)}\")\n",
    "\n",
    "gene_selected = dataset.x_expression.columns & gene_names\n",
    "X_train = dataset.x_expression.loc[train_idx, gene_selected]\n",
    "Y_train = dataset.Y.loc[train_idx].values.reshape(-1)\n",
    "\n",
    "\n",
    "X_test = test_dataset.x_expression.loc[test_idx, gene_selected]\n",
    "Y_test = test_dataset.Y.loc[test_idx].values.reshape(-1)\n",
    "\n",
    "# Fit logistic regression classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(penalty=\"l2\", C=0.01, random_state=n_seed)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Training set\")\n",
    "Y_pred = clf.predict(X_train)\n",
    "Y_proba = clf.decision_function(X_train)\n",
    "fpr, tpr, thres = roc_curve(Y_train, Y_proba)\n",
    "cutoff = thres[np.argmin(1-tpr+fpr)]\n",
    "Y_pred = [1 if x > cutoff else 0 for x in Y_proba]\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, precision_recall_fscore_support\n",
    "acc = accuracy_score(Y_train, Y_pred)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "scores = precision_recall_fscore_support(Y_train, Y_pred, average=\"binary\")\n",
    "print(f\"Precision: {scores[0]:.3f}\")\n",
    "print(f\"Recall: {scores[1]:.3f}\")\n",
    "print(f\"F1 score: {scores[2]:.3f}\")\n",
    "print(\"--------------\")\n",
    "\n",
    "print(\"Test set\")\n",
    "Y_pred = clf.predict(X_test)\n",
    "Y_proba = clf.decision_function(X_test)\n",
    "Y_pred = [1 if x > cutoff else 0 for x in Y_proba]\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, precision_recall_fscore_support\n",
    "acc = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n",
    "scores = precision_recall_fscore_support(Y_test, Y_pred, average=\"binary\")\n",
    "print(f\"Precision: {scores[0]:.3f}\")\n",
    "print(f\"Recall: {scores[1]:.3f}\")\n",
    "print(f\"F1 score: {scores[2]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba = clf.predict_proba(X_test)\n",
    "from sklearn.metrics import plot_precision_recall_curve, roc_curve, auc\n",
    "plot_precision_recall_curve(clf, X_test, Y_test)\n",
    "\n",
    "# ROC\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(2):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Y_test, Y_proba[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[1], tpr[1], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[1])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([-0.02, 1.0])\n",
    "plt.ylim([0.0, 1.02])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Genes in pathways\n",
    "pathway_names = path_graphs.pathway_list\n",
    "#pathway_names = list(shap_value_df.mean().sort_values(ascending=False)[:5].index)\n",
    "gene_names = set()\n",
    "for p in pathway_names:\n",
    "    gene_names = gene_names | set(path_graphs.id_to_symbol[p].values())\n",
    "print(len(gene_names))\n",
    "#print(gene_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_curve, precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    "\n",
    "# preferences\n",
    "n_graphs = 5\n",
    "n_start = 0\n",
    "\n",
    "score_dict = {\"label\": [],\n",
    "    \"acc\": [],\n",
    "         \"precision\": [],\n",
    "         \"recall\": [],\n",
    "         \"f1score\": [],\n",
    "         \"rocauc\": [],\n",
    "         \"acc_test\":[],\n",
    "         \"prec_test\":[],\n",
    "         \"rec_test\":[],\n",
    "         \"f1_test\":[],\n",
    "         \"rocauc_test\":[]}\n",
    "\n",
    "while n_start < shap_value_df.shape[1]:\n",
    "    print(\"=================\")\n",
    "    print(f\"n_start: {n_start}\")\n",
    "    print(\"-----------------\")\n",
    "    # Select genes in the pathways\n",
    "    pathway_names = list(shap_value_df.mean().sort_values(ascending=False)[n_start:n_start+n_graphs].index)\n",
    "    gene_names = set()\n",
    "    for p in pathway_names:\n",
    "        gene_names = gene_names | set(path_graphs.id_to_symbol[p].values())\n",
    "\n",
    "    gene_selected = dataset.x_expression.columns & gene_names\n",
    "    X_train = dataset.x_expression.loc[train_idx, gene_selected]\n",
    "    Y_train = dataset.Y.loc[train_idx].values.reshape(-1)\n",
    "\n",
    "    X_test = test_dataset.x_expression.loc[test_idx, gene_selected]\n",
    "    Y_test = test_dataset.Y.loc[test_idx].values.reshape(-1)\n",
    "\n",
    "    # Fit logistic regression classifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(penalty=\"l2\", C=0.01, random_state=n_seed)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Training set\")\n",
    "    score_dict[\"label\"].append(n_start)\n",
    "    Y_pred = clf.predict(X_train)\n",
    "    Y_proba = clf.decision_function(X_train)\n",
    "    fpr, tpr, thres = roc_curve(Y_train, Y_proba)\n",
    "    cutoff = thres[np.argmin(1-tpr+fpr)]\n",
    "    Y_pred = [1 if x > cutoff else 0 for x in Y_proba]\n",
    "    \n",
    "\n",
    "    # evaluation\n",
    "    acc = accuracy_score(Y_train, Y_pred)\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    scores = precision_recall_fscore_support(Y_train, Y_pred, average=\"binary\")\n",
    "    \n",
    "    print(f\"Precision: {scores[0]:.3f}\")\n",
    "    print(f\"Recall: {scores[1]:.3f}\")\n",
    "    print(f\"F1 score: {scores[2]:.3f}\")\n",
    "    score_dict[\"acc\"].append(acc)\n",
    "    score_dict[\"precision\"].append(scores[0])\n",
    "    score_dict[\"recall\"].append(scores[1])\n",
    "    score_dict[\"f1score\"].append(scores[2])\n",
    "    \n",
    "    roc_auc = roc_auc_score(Y_train, Y_proba)\n",
    "    score_dict[\"rocauc\"].append(roc_auc)\n",
    "    print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "    \n",
    "    print(\"--------------\")\n",
    "\n",
    "    print(\"Test set\")\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    Y_proba = clf.decision_function(X_test)\n",
    "    Y_pred = [1 if x > cutoff else 0 for x in Y_proba]\n",
    "\n",
    "    acc = accuracy_score(Y_test, Y_pred)\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    scores = precision_recall_fscore_support(Y_test, Y_pred, average=\"binary\")\n",
    "    print(f\"Precision: {scores[0]:.3f}\")\n",
    "    print(f\"Recall: {scores[1]:.3f}\")\n",
    "    print(f\"F1 score: {scores[2]:.3f}\")\n",
    "    score_dict[\"acc_test\"].append(acc)\n",
    "    score_dict[\"prec_test\"].append(scores[0])\n",
    "    score_dict[\"rec_test\"].append(scores[1])\n",
    "    score_dict[\"f1_test\"].append(scores[2])\n",
    "    roc_auc = roc_auc_score(Y_test, Y_proba)\n",
    "    score_dict[\"rocauc_test\"].append(roc_auc)\n",
    "    print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "    \n",
    "    n_start += n_graphs\n",
    "    \n",
    "# Save data\n",
    "df = pd.DataFrame(score_dict)\n",
    "df.to_csv(os.path.join(file_path, \"pathway_rank_log_reg_score.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(os.path.join(file_path, \"pathway_rank_log_reg_score.csv\"), index_col=0)\n",
    "df = df.loc[:, [\"label\", \"acc_test\", \"prec_test\", \"rec_test\", \"f1_test\"]]\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 6))\n",
    "plt.bar(df[\"label\"].astype(\"str\"), df[\"f1_test\"])\n",
    "plt.title(\"F1 score of test set with gene selection by pathway rank\")\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_xlabel(\"Pathway rank\")\n",
    "ax.set_ylabel(\"F1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot\n",
    "df = pd.read_csv(os.path.join(file_path, \"pathway_rank_log_reg_score.csv\"), index_col=0)\n",
    "\n",
    "df = df.loc[:, [\"label\", \"acc_test\", \"prec_test\", \"rec_test\", \"f1_test\"]]\n",
    "\n",
    "# Line fitting\n",
    "ab = np.polyfit(df[\"label\"],df[\"f1_test\"],1)\n",
    "y1 = np.poly1d(ab)(df[\"label\"])\n",
    "\n",
    "print(sklearn.metrics.r2_score(df[\"f1_test\"], y1))\n",
    "\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 6))\n",
    "plt.plot(df[\"label\"], df[\"f1_test\"], marker='o', linestyle=\"None\", label=\"F1 score\")\n",
    "plt.plot(df[\"label\"], y1, marker=None, linestyle=\"dashed\", label=\"linear regression\")\n",
    "plt.title(\"F1 score of test set with gene selection by pathway rank\")\n",
    "plt.legend(loc='lower left', fontsize=10, framealpha=1.0, facecolor=\"w\")\n",
    "#plt.xticks(rotation=90)\n",
    "ax.set_xlabel(\"Pathway rank\")\n",
    "ax.set_ylabel(\"F1 score\")\n",
    "plt.savefig(os.path.join(file_path, \"log_reg_f1score_pathway_rank.eps\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gene level SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap_value_df = pd.read_csv(os.path.join(file_path, \"kegg_shap.csv\"), index_col=0)\n",
    "shap_value_df = pd.read_csv(os.path.join(file_path, \"gene_level_shap.csv\"), index_col=0)\n",
    "shap_cols = [col.split(\".\")[0] for col in shap_value_df.columns]\n",
    "shap_value_df = shap_value_df.abs().T.groupby(shap_cols).sum().T\n",
    "shap.summary_plot(shap_value_df, feature_names=shap_value_df.columns, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_genes = 100\n",
    "n_start = 0\n",
    "\n",
    "score_dict = {\"label\": [],\n",
    "    \"acc\": [],\n",
    "         \"precision\": [],\n",
    "         \"recall\": [],\n",
    "         \"f1score\": [],\n",
    "         \"rocauc\": [],\n",
    "         \"acc_test\":[],\n",
    "         \"prec_test\":[],\n",
    "         \"rec_test\":[],\n",
    "         \"f1_test\":[],\n",
    "         \"rocauc_test\":[]}\n",
    "\n",
    "while n_start < shap_value_df.shape[1]:\n",
    "\n",
    "    print(\"=================\")\n",
    "    print(f\"n_start: {n_start}\")\n",
    "    print(\"-----------------\")\n",
    "    # Select genes\n",
    "    gene_names = list(shap_value_df.mean().sort_values(ascending=False)[n_start:n_start+n_genes].index)\n",
    "    #gene_names = list(shap_value_df.mean().sort_values(ascending=False).index)\n",
    "    gene_selected = dataset.x_expression.columns & gene_names\n",
    "    X_train = dataset.x_expression.loc[train_idx, gene_selected]\n",
    "    Y_train = dataset.Y.loc[train_idx].values.reshape(-1)\n",
    "\n",
    "    X_test = test_dataset.x_expression.loc[test_idx, gene_selected]\n",
    "    Y_test = test_dataset.Y.loc[test_idx].values.reshape(-1)\n",
    "    \n",
    "    if not X_train.shape[1] > 0:\n",
    "        break\n",
    "\n",
    "    # Fit logistic regression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(penalty=\"l2\", C=0.01, random_state=n_seed)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Training set\")\n",
    "    score_dict[\"label\"].append(n_start)\n",
    "    Y_pred = clf.predict(X_train)\n",
    "    Y_proba = clf.decision_function(X_train)\n",
    "    fpr, tpr, thres = roc_curve(Y_train, Y_proba)\n",
    "    cutoff = thres[np.argmin(1-tpr+fpr)]\n",
    "    Y_pred = [1 if x > cutoff else 0 for x in Y_proba]\n",
    "\n",
    "\n",
    "    # Evaluation\n",
    "    acc = accuracy_score(Y_train, Y_pred)\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    scores = precision_recall_fscore_support(Y_train, Y_pred, average=\"binary\")\n",
    "\n",
    "    print(f\"Precision: {scores[0]:.3f}\")\n",
    "    print(f\"Recall: {scores[1]:.3f}\")\n",
    "    print(f\"F1 score: {scores[2]:.3f}\")\n",
    "    score_dict[\"acc\"].append(acc)\n",
    "    score_dict[\"precision\"].append(scores[0])\n",
    "    score_dict[\"recall\"].append(scores[1])\n",
    "    score_dict[\"f1score\"].append(scores[2])\n",
    "\n",
    "    roc_auc = roc_auc_score(Y_train, Y_proba)\n",
    "    score_dict[\"rocauc\"].append(roc_auc)\n",
    "    print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "\n",
    "    print(\"--------------\")\n",
    "\n",
    "    print(\"Test set\")\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    Y_proba = clf.decision_function(X_test)\n",
    "    Y_pred = [1 if x > cutoff else 0 for x in Y_proba]\n",
    "\n",
    "    acc = accuracy_score(Y_test, Y_pred)\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    scores = precision_recall_fscore_support(Y_test, Y_pred, average=\"binary\")\n",
    "    print(f\"Precision: {scores[0]:.3f}\")\n",
    "    print(f\"Recall: {scores[1]:.3f}\")\n",
    "    print(f\"F1 score: {scores[2]:.3f}\")\n",
    "    score_dict[\"acc_test\"].append(acc)\n",
    "    score_dict[\"prec_test\"].append(scores[0])\n",
    "    score_dict[\"rec_test\"].append(scores[1])\n",
    "    score_dict[\"f1_test\"].append(scores[2])\n",
    "    roc_auc = roc_auc_score(Y_test, Y_proba)\n",
    "    score_dict[\"rocauc_test\"].append(roc_auc)\n",
    "    print(f\"ROC AUC: {roc_auc:.3f}\")\n",
    "    \n",
    "    \n",
    "    n_start += n_genes\n",
    "\n",
    "# データを保存する\n",
    "df = pd.DataFrame(score_dict)\n",
    "df.to_csv(os.path.join(file_path, \"gene_rank_log_reg_score.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv(os.path.join(file_path, \"gene_rank_log_reg_score.csv\"), index_col=0)\n",
    "\n",
    "df = df.loc[:, [\"label\", \"acc_test\", \"prec_test\", \"rec_test\", \"f1_test\"]]\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 6))\n",
    "plt.bar(df[\"label\"].astype(\"str\"), df[\"f1_test\"])\n",
    "plt.title(\"F1 score of test set with gene selection by gene rank\")\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_xlabel(\"Gene rank\")\n",
    "ax.set_ylabel(\"F1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot and linear regression\n",
    "# Load data\n",
    "df = pd.read_csv(os.path.join(file_path, \"gene_rank_log_reg_score.csv\"), index_col=0)\n",
    "df = df.loc[:, [\"label\", \"acc_test\", \"prec_test\", \"rec_test\", \"f1_test\"]]\n",
    "\n",
    "# Linear regression\n",
    "ab = np.polyfit(df[\"label\"],df[\"f1_test\"],1)\n",
    "y1 = np.poly1d(ab)(df[\"label\"])\n",
    "\n",
    "print(sklearn.metrics.r2_score(df[\"f1_test\"], y1))\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 6))\n",
    "plt.plot(df[\"label\"], df[\"f1_test\"], marker='o', linestyle=\"None\", label=\"F1 score\")\n",
    "plt.plot(df[\"label\"], y1, marker=None, linestyle=\"dashed\", label=\"linear regression\")\n",
    "plt.title(\"F1 score of test set with gene selection by gene rank\")\n",
    "plt.legend(loc='lower left', fontsize=10, framealpha=1.0, facecolor=\"w\")\n",
    "ax.set_xlabel(\"Gene rank\")\n",
    "ax.set_ylabel(\"F1 score\")\n",
    "plt.savefig(os.path.join(file_path, \"log_reg_f1score_gene_rank.eps\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
